{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "45e4a2b7-7394-4eda-9220-12fe12572e52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "use catalog customer_catalog;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88d76a0a-aa71-446d-b0f5-10b779147955",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Importing necessary PySpark DataFrame, functions and types for data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a1ad982e-39ec-4ed7-ba3b-dd6bc486fc57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import explode, col, cast, when, to_date, try_to_timestamp, explode_outer\n",
    "from pyspark.sql.types import IntegerType, FloatType, DateType, DoubleType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b2518164-a3a8-4e5d-a12b-c6ecaaf21a9e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Read Function for json file format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd4148e3-aa3d-41c1-8cb0-215d2831a3d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "''' def read_file(layer, table_name):\n",
    "    return spark.read.format(\"json\") \\\n",
    "                     .option(\"multiline\", \"true\") \\\n",
    "                     .table(f\"customer_catalog.{layer}.{table_name}\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "39f64335-da1d-4b3a-91de-be27ab67693e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Read function for any file format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "822242bb-11de-435b-9592-1669a9477cdd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This function reads a file from the specified layer and table name.\n",
    "Parameters :\n",
    "- layer : Data Layer [bronze, silver, gold]\n",
    "- table_name : Name of the table\n",
    "- fileFormat : format of the file [json, CSV, parquet, delta]\n",
    "- header : if true, first row is header (default is False)\n",
    "- inferSchema : if true, take schema from the file (default is false)\n",
    "- multiLine : used for JSON file, (default is False)\n",
    "- mode : [PERMISSIVE : put the corrupt data in corrupt_Record column and put null in all the columns,          DROPMALFROMED : drops the row that has corrupted data, FAILFAST : fails to read the file that has corrupted data]\n",
    "(default is PERMISSIVE)\n",
    "- columnnameofCorruptedRecord : name of the column which stores the corrupted data\n",
    "- delimiter : used for CSV files, the symbol used to separate fields (default is \",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6637babf-8840-4b6e-8945-58e4432c0fef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "897a6c39-663e-4b9a-bc36-5050e2d1e9e4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "After getting the fileFormat as input it returns DataFrame according to that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "91f32683-49e3-4045-9786-a8b834ae9478",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_file(layer : str,\n",
    "              table_name : str,\n",
    "              fileFormat : str,\n",
    "              header : bool = False,\n",
    "              inferSchema : bool = False,\n",
    "              multiLine : bool = False,\n",
    "              mode : str = \"PERMISSIVE\",\n",
    "              columnnameofCorruptedRecord : str = \"corrupted_record\",\n",
    "              delimiter : str = \",\"\n",
    "              ) -> DataFrame:\n",
    "    \n",
    "    file_path = (f\"customer_catalog.{layer}.{table_name}\")\n",
    "    \n",
    "    if fileFormat == \"json\" :\n",
    "        return spark.read.format(\"json\") \\\n",
    "                  .option(\"multiLine\", multiLine) \\\n",
    "                  .table(file_path)\n",
    "\n",
    "    elif fileFormat == \"csv\" :\n",
    "        return spark.read.format(\"csv\") \\\n",
    "                  .option(\"header\", header) \\\n",
    "                  .option(\"inferSchema\", inferSchema) \\\n",
    "                  .option(\"delimiter\", delimiter) \\\n",
    "                  .table(file_path)\n",
    "    \n",
    "    elif fileFormat == \"parquet\" :\n",
    "        return spark.read.format(\"parquet\") \\\n",
    "                  .table(file_path)\n",
    "    \n",
    "    elif fileFormat == \"delta\" :\n",
    "        return spark.read.format(\"delta\") \\\n",
    "                  .table(file_path)\n",
    "    else :\n",
    "        print(\"Invalid file format\")\n",
    "                  \n",
    "   # return (f\"df_{layer}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "824b7940-4b61-4d31-b422-6457053077cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Function to flatten struct and array type column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "762b2cf2-6c01-4858-b79a-171950dd0e3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "def flatten_struct(df, col_name, prefix=\"\"):\n",
    "    fields = df.schema[col_name].dataType.fields\n",
    "    for field in fields:\n",
    "        field_name = f\"{prefix}{col_name}_{field.name}\"\n",
    "        if field.dataType.typeName() == \"struct\":\n",
    "            df = flatten_struct(df.withColumn(field_name, col(f\"{col_name}.{field.name}\")), field_name)\n",
    "        else:\n",
    "            df = df.withColumn(field_name, col(f\"{col_name}.{field.name}\"))\n",
    "    return df.drop(col_name)\n",
    "\n",
    "df = read_file(\"bronze\", \"orders\", \"json\")\n",
    "\n",
    "while True :\n",
    "    for name, dtype in df.dtypes:\n",
    "       field = df.schema[name]\n",
    "       if field.dataType.typeName() == \"array\":\n",
    "           df = df.withColumn(name, explode(col(name)))\n",
    "       elif field.dataType.typeName() == \"struct\":\n",
    "           df = flatten_struct(df, name)\n",
    "display(df)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aeaa36ad-3132-460a-9a7e-c68af54fb755",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def flatten_struct(df : DataFrame,\n",
    "                   col_name : str) -> DataFrame :\n",
    "    fields = df.schema[col_name].dataType.fields\n",
    "    for field in fields:\n",
    "        field_name = f\"{col_name}_{field.name}\"\n",
    "        if field.dataType.typeName() == \"struct\":\n",
    "            df = flatten_struct(df.withColumn(field_name, col(f\"{col_name}.{field.name}\")), field_name)\n",
    "        else:\n",
    "            df = df.withColumn(field_name, col(f\"{col_name}.{field.name}\"))\n",
    "    return df.drop(col_name)\n",
    "\n",
    "def flatten_column(df : DataFrame) -> DataFrame:\n",
    "    while True:\n",
    "        complex_fields = []\n",
    "        for field in df.schema.fields:\n",
    "            if field.dataType.typeName() == \"array\":\n",
    "                complex_fields.append((field.name, \"array\"))\n",
    "            elif field.dataType.typeName() == \"struct\":\n",
    "                complex_fields.append((field.name, \"struct\"))\n",
    "\n",
    "        if len(complex_fields) == 0:\n",
    "            break\n",
    "\n",
    "        for field_name, field_type in complex_fields:\n",
    "            if field_type == \"array\":\n",
    "                df = df.withColumn(field_name, explode_outer(col(field_name)))\n",
    "            elif field_type == \"struct\":\n",
    "                df = flatten_struct(df, field_name)\n",
    "                \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf5d96d1-8d22-4ba0-8da1-d38c329c419a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Write function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "572bc1ac-60f9-4e29-b3a5-f7b96862e3d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This function write the table in delta format.\n",
    "Parameters :\n",
    "- layer : Data layer [bronze, silver, gold]\n",
    "- table_name : name of the table\n",
    "- mode : [append - add new columns after existing data, overwrite - deletes exsiting data and put the new data, error - throws error if table already exists (default), ignore - ignore if table already exists]\n",
    "- overwriteSchema - if true, it will replace the existing table schema with the new one (works with overwrite mode)\n",
    "- mergeSchema - if true, it will merge the existing and new schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84f61904-d22a-41ce-9453-0f6a1272bf7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def write_file(layer : str,\n",
    "               table_name : str,\n",
    "               mode : str,\n",
    "               overwriteSchema : bool = False,\n",
    "               mergeSchema : bool = False,\n",
    "               ):\n",
    "    return spark.write.format(\"delta\") \\\n",
    "                   .option(\"mode\", mode) \\\n",
    "                   .option(\"overwriteSchema\", overwriteSchema) \\\n",
    "                   .option(\"mergeSchema\", mergeSchema) \\\n",
    "                   .saveAsTable(f\"customer_catalog.{layer}.{table_name}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": null
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4915869172309322,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "utils",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
